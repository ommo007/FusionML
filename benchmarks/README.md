# FusionML Benchmarks

Community-driven benchmarks for FusionML across Apple Silicon devices.

## ğŸš€ Quick Start

Just run the shell script - it handles everything!

```bash
cd benchmarks
./run_benchmarks.sh
```

This will:
1. âœ… Create a virtual environment
2. âœ… Install all dependencies (numpy, mlx, torch)
3. âœ… Run all 4 benchmarks
4. âœ… Save results in a device-specific folder

## ğŸ“Š Results Structure

Results are organized by device:

```
results/
â”œâ”€â”€ M1_8GB_8cores_926Gi/
â”‚   â”œâ”€â”€ matmul.json
â”‚   â”œâ”€â”€ training.json
â”‚   â”œâ”€â”€ vs_mlx.json
â”‚   â””â”€â”€ vs_pytorch.json
â”œâ”€â”€ M2_Pro_16GB_12cores_500Gi/
â”‚   â”œâ”€â”€ matmul.json
â”‚   â””â”€â”€ ...
â””â”€â”€ M4_24GB_10cores_1Ti/
    â””â”€â”€ ...
```

Each device folder contains 4 JSON files:
- **matmul.json** - Matrix multiplication (256 to 4096)
- **training.json** - MLP training benchmark
- **vs_mlx.json** - FusionML vs MLX comparison
- **vs_pytorch.json** - FusionML vs PyTorch comparison

## ğŸ“ Submit Your Results

1. Fork this repository
2. Run `./run_benchmarks.sh`
3. Create a PR with your device folder from `results/`

## ğŸ”§ Manual Setup

```bash
cd benchmarks/python

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -e ../../python

# Run all benchmarks
python run_benchmark.py
```

## ğŸ† Hall of Fame

Contributors who submitted verified benchmarks:

- *Your name here!*

---

*FusionML - Faster than MLX on Apple Silicon* ğŸš€
